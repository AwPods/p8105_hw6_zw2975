---
title: "p8105_hw6_zw2975"
author: "Zhiyu Wei"
date: 2024-12-1
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(patchwork)
library(knitr)
library(modelr)
library(mgcv)
library(SemiPar)
library(labelled)
```


## Problem 1

```{r obtaining dataset}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```


```{r fit model and boostrapping}
n_sample = 5000

# function for sampling
boot_sample = function(df) {
  sample_frac(df, replace = TRUE)
}


boot_straps = 
  tibble(strap_number = 1:n_sample) |> 
  mutate(
    strap_sample = map(strap_number, ~ boot_sample(df = weather_df))
  )

# linear regression fit
extract = function(df) {
  model = lm(tmax ~ tmin, data = df)
  r_squared = broom::glance(model)$r.squared # Extract R square
  beta = broom::tidy(model) # Extract coefficients and calculate log(β0 * β1)
  log_beta = log(beta$estimate[1] * beta$estimate[2])
  tibble(r_squared = r_squared, log_beta = log_beta)
}

# Apply the function to each bootstrap sample
boot_results = boot_straps |> 
  mutate(
    results = purrr::map(strap_sample, extract)
  ) |> 
  unnest(results)

```



```{r plot estimates}
# Plot R square distribution
ggplot(boot_results, aes(x = r_squared)) +
  geom_histogram(bins = 30, fill = "blue", alpha = 0.6) +
  labs(
    title = "Bootstrap Distribution of R2 Estimates",
    x = "R squared",
    y = "Frequency"
  )

# Plot log(β0 * β1) distribution
ggplot(boot_results, aes(x = log_beta)) +
  geom_histogram(bins = 30, fill = "red", alpha = 0.6) +
  labs(
    title = "Distribution of log(β0 * β1) estimates",
    x = "log(β0 * β1)",
    y = "Frequency"
  ) 
```

###### Describe in word

Both distributions are normally distributed. The R squared distribution has the highest number of observations around 0.91 that has slightly less than 500 observations. The highest R squared does not exceed 0.94 and the lowest does not go lower than 0.87. This also suggests that there is a high linear correlation between tmin and tmax in the linear regression model. 

For the log(B0*B1) distribution, the highest log(B0*B1) lies around 2.2 with more than 500 observations. The highest observation does not exceed 2.125 and the lowest does not go under 1.925. 

```{r confidence interval}
ci_r_squared = quantile(boot_results$r_squared, c(0.025, 0.975))
ci_log_beta = quantile(boot_results$log_beta, c(0.025, 0.975))

print(ci_r_squared)
print(ci_log_beta)

```


## Problem 2
```{r import data}
# import data
homic = read.csv("./data/homicide-data.csv")
```

```{r data manipulation, warning = FALSE}
# create city_state variable, omit places, only keep desired races, numeric victim_age
homic = mutate(homic,
           city_state= paste(city, state, sep=', ')) |>
filter(!(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")))|>
  filter(victim_race %in% c("White", "Black")) |>
  mutate(victim_age = as.numeric(victim_age))

# create binary for case status
homic = homic |>
  mutate(resolved = ifelse(disposition == "Closed by arrest", 1, 0))

# create a Baltimore dataset
balt = homic |>
  filter(city_state == "Baltimore, MD")
```

#### Logistic Regression Model

```{r fit logistic regression}
# predictors are sex, age, and race of victims (in Baltimore, MD)
balt_fit = glm(resolved ~ victim_age + victim_race + victim_sex, data = balt, family = binomial()) 

# Extracing CIs and OR
balt_results = broom::tidy(balt_fit, conf.int = TRUE) |>
  filter(term == "victim_sexMale") |>
  mutate(odds_ratio = exp(estimate),
         conf.low = exp(conf.low),
         conf.high = exp(conf.high))

print(balt_results)
```


```{r each city, warning = FALSE}
city_results = homic |>
  group_by(city_state) |>
  nest() |>
  mutate(
    model = purrr::map(data, ~ glm(resolved ~ victim_age + victim_sex + victim_race, 
                            data = ., 
                            family = binomial)),
    tidy_model = purrr::map(model, ~ broom::tidy(.x, conf.int = TRUE)),
    odds_ratios = purrr::map(tidy_model, ~ .x |>
                        filter(term == "victim_sexMale") |>
                        mutate(odds_ratio = exp(estimate),
                               conf.low = exp(conf.low),
                               conf.high = exp(conf.high)))
  ) |>
  unnest(odds_ratios) |>
  select(city_state, odds_ratio, conf.low, conf.high)
```


#### Plot of OR and CIs

```{r plot city results}
ggplot(city_results, aes(x = reorder(city_state, odds_ratio), y = odds_ratio)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2, color = "gray") + 
  labs(
    title = "Adjusted Odds Ratios and CIs for Resolved Homicides by City",
    y = "Odds Ratio and CIs (male compared to female)",
    x = "City") +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1)) # Rotate city names to save space

```

###### Comment
New York has the lowest odds ratio and also a relatively small range of CI for the odds ratio. It is also obvious that the CI increases in size as the odds ratio increases for most cities. The highest odds ratio of male vs. female resolved crime is from Albuquerque, NM. The highest odds ratio does not exceed 2 and most of the odds ratio lies between 0 and 1, which means that there are more resolved crime when victim's sex is female for most cases. 


## Problem 3

```{r Import p3 data}
bw = read.csv("./data/birthweight.csv")
```


```{r clean dataset}
# changed numbers to labelled entries
bw = bw |>
set_value_labels(
  frace = c("White" = 1, "Black" = 2, "Asian" = 3, "Puerto Rican" = 4, "Other" = 8, "Unknown" = 9),
  mrace = c("White" = 1, "Black" = 2, "Asian" = 3, "Puerto Rican" = 4, "Other" = 8, "Unknown" = 9))

# changed labelled entries to factor
bw = bw |>
  mutate_if(is.labelled, to_factor)
```


```{r fitted vs. residuals}
# Regression model
bw_model = lm(bwt ~ gaweeks + delwt + smoken + babysex + mrace * frace, data = bw)

# Add predictions and residuals to the data
bw = bw |>
  add_predictions(bw_model, var = "fitted") |>
  add_residuals(bw_model, var = "residuals")

# Plot residuals against fitted values
ggplot(bw, aes(x = fitted, y = residuals)) +
  geom_point(alpha = 0.3) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Residuals vs. Fitted Values",
    x = "Fitted Values",
    y = "Residuals")

```


```{r comparison models}
model1 = lm(bwt ~ blength + gaweeks, data = bw)

model2 = lm(bwt ~ bhead * blength * babysex, data = bw)
```


```{r cv functions}
cv_df <- crossv_mc(bw, n = 100, test = 0.2)

# convert train and test sets to tibbles
cv_df <- cv_df |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

# fit models on training data
cv_df <- cv_df |> 
  mutate(
    bw_mod = map(train, ~ lm(bwt ~ gaweeks + delwt + smoken + babysex + mrace, data = .x)),
    m1_mod = map(train, ~ gam(bwt ~ s(blength) + s(gaweeks), data = .x)),
    m2_mod = map(train, ~ gam(bwt ~ s(bhead, blength, babysex), data = .x)))

# function to calculate RMSE
rmse <- function(model, data) {
  preds <- predict(model, newdata = data)
  sqrt(mean((data$bwt - preds)^2))
}

# get RMSE for every model
cv_df <- cv_df |> 
  mutate(
    rmse_bw = map2_dbl(bw_mod, test, ~ rmse(.x, .y)),
    rmse_m1 = map2_dbl(m1_mod, test, ~ rmse(.x, .y)),
    rmse_m2 = map2_dbl(m2_mod, test, ~ rmse(.x, .y)))


# summarize RMSE for every model
rmse_summary <- cv_df |> 
  summarise(
    mean_rmse_bw = mean(rmse_bw),
    mean_rmse_m1 = mean(rmse_m1),
    mean_rmse_m2 = mean(rmse_m2),
    sd_rmse_bw = sd(rmse_bw),
    sd_rmse_m1 = sd(rmse_m1),
    sd_rmse_m2 = sd(rmse_m2))

print(rmse_summary)
```

###### Comment

The summarized RMSE has proven that Model 2, which uses head circumference, length, sex, and all interactions (including the three-way interaction), has the best fit on the data. The RMSE is the second lowest for Model 1, which uses length at birth and gestational age as predictors with only the main effects. My model, however, has the highest RMSE, which means it has the worst fit. It would probably be better to add more interaction terms to the model for a better fit over the data. 


