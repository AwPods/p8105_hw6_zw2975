---
title: "p8105_hw6_zw2975"
author: "Zhiyu Wei"
date: 2024-11-14
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(patchwork)
library(knitr)
```


## Problem 1

```{r obtaining dataset}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

```{r linear model}
fit = lm(tmax ~ tmin, data = weather_df)
#fit model and get b0 and b1

summary(fit)
```

```{r bootstrapping}
n_samp = 5000

sim_df_const = 
  tibble(
    x = rnorm(n_samp, 1, 1),
    error = rnorm(n_samp, 0, 1),
    y = 2 + 3 * x + error
  )

sim_df_nonconst = sim_df_const |> 
  mutate(
  error = error * .75 * x,
  y = 2 + 3 * x + error
)

```

```{r plot samples}
sim_df = 
  bind_rows(const = sim_df_const, nonconst = sim_df_nonconst, .id = "data_source") 

sim_df |> 
  ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5) +
  stat_smooth(method = "lm") +
  facet_grid(~data_source) 
```


## Problem 2
```{r p2}
# set fixed design elements
n = 30
s = 5

set.seed(1)  # set seed

# Function to simulate data and run t-test
power_sim = function(mu, n = 30, sigma = 5, num_sim = 5000, alpha = 0.05) {
  results = replicate(num_sim, {
    x = rnorm(n, mean = mu, sd = s)
    t_test = t.test(x, mu = 0)
    broom::tidy(t_test)
  }, simplify = FALSE)
  
  results = bind_rows(results) |>
    mutate(true_mu = mu)
  
  return(results)
}

# Simulate for different values of mu
mu_values = 0
sim_results = bind_rows(lapply(mu_values, power_sim))

mu_values = 0:6
sim_results = bind_rows(lapply(mu_values, power_sim))

# Calculate power and mean estimates for each mu
power_results = sim_results |>
  group_by(true_mu) |>
  summarise(power = mean(p.value < 0.05),  # Proportion of rejections
    avg_estimate = mean(estimate),
    avg_estimate_rejected = mean(estimate[p.value < 0.05]))

```

##### plot 1

```{r plot 1}
ggplot(power_results, aes(x = true_mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Power vs. True μ",
    x = "True μ",
    y = "Power (Proportion of Null Rejections)"
  )
```


##### plot 2

```{r plot 2, warning = FALSE}
ggplot(power_results, aes(x = true_mu)) +
  geom_line(aes(y = avg_estimate), color = "blue", linetype = "dashed") +
  geom_line(aes(y = avg_estimate_rejected), color = "red") +
  geom_point(aes(y = avg_estimate), color = "blue") +
  geom_point(aes(y = avg_estimate_rejected), color = "red") +
  labs(
    title = "Average Estimate of μ vs. True μ
    (red = Avereage estimate rejected, blue = Average estimate of μ)",
    x = "True μ",
    y = "Average Estimate of μ") 

```

##### Comment

The average estimate of μ across all samples (blue dashed line) closely follows the true value of μ, indicating unbiased estimation. 

The red solid line (representing estimates from samples where the null was rejected) shows a slight upward increase between 0 to 3 for the True μ.

This bias arises because samples with larger effect sizes are more likely to reject the null hypothesis, where only extreme estimates are considered. That is the reason why the sample average of μ in cases where the null hypotheses are rejected slightly exceeds the true μ.

## Problem 3

```{r P3}
# import data
homic = read.csv("./data files/homicide-data.csv")
```

###### Description of raw data

The dataset contains `r ncol(homic)` and `r nrow(homic)` records of homicides from `50` major U.S. cities, with details on each case such as victim demographics (`age`, `race`, `sex`), location (`city`, `state`, `latitude`, `longitude`), and case status (`Closed by arrest`, `Closed without arrest`, or `Open/No arrest`). 
```{r data manipulation}
# create city_state variable
homic = mutate(homic,
           city_state= paste(city, state, sep=', '))

# get summary data
summary = homic |> 
  group_by(city_state)|>
  summarise(
    Total_homicides = n(),
    Unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )
```


```{r prop test}
# only leaves out Baltimore, MD
balt_summary = summary |>
  filter(city_state =="Baltimore, MD")

# Apply prop.test
bal_test = prop.test(balt_summary$Unsolved_homicides, balt_summary$Total_homicides)

# broom::tidy the results
Bal_tidy = broom::tidy(bal_test)

# Extract proportion and confidence interval
balt_results = Bal_tidy |>
  select(estimate, conf.low, conf.high)
print(balt_results)

```


```{r running prop test for each city, warning=FALSE}

### original
city_results = summary |>
  mutate(
    test_results = map2(Unsolved_homicides, Total_homicides, ~ prop.test(.x, .y))
  ) |>
  mutate(tidy_results = map(test_results, broom::tidy)) |>
  unnest(tidy_results)|>
  select(city_state, estimate, conf.low, conf.high, p.value)

```


```{r Problem 3 plot}
#sort out proportions of unsolved homicides
city_results = city_results |>
  arrange(desc(estimate))
# Plot
ggplot(city_results, aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point(size = 3, color = "red") +  
  # error bar chart
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2, color = "gray") +
  labs(
    title = "Proportion of unsolved homicides by city",
    x = "City",
    y = "Estimated proportion of unsolved Homicides (high to low)"
  ) + 
  coord_flip()   # Flip the coordinates for better view
```

